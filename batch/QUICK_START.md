# ğŸš€ QUICK START - Batch Pipeline

## ğŸ“‹ YÃªu cáº§u trÆ°á»›c khi cháº¡y
- Docker Desktop Ä‘Ã£ cÃ i Ä‘áº·t vÃ  Ä‘ang cháº¡y
- Python 3.8+ Ä‘Ã£ cÃ i Ä‘áº·t
- ÄÃ£ cÃ i Ä‘áº·t dependencies: `pip install -r requirements.txt`

---

## ğŸ¯ CÃCH CHáº Y PIPELINE

### **BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng Docker Services**

Má»Ÿ **Terminal/PowerShell**, cháº¡y:

```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ containers (Kafka, MinIO, Elasticsearch, Kibana)
docker-compose up -d

# Äá»£i 30 giÃ¢y Ä‘á»ƒ Kafka khá»Ÿi Ä‘á»™ng hoÃ n toÃ n
timeout /t 30

# Kiá»ƒm tra containers Ä‘ang cháº¡y
docker ps
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
CONTAINER ID   IMAGE                         STATUS    PORTS
xxx            confluentinc/cp-kafka         Up        9092-9093
xxx            confluentinc/cp-zookeeper     Up        2181
xxx            minio/minio                   Up        9000-9001
xxx            elasticsearch:8.18.8          Up        9200,9300
xxx            kibana:8.18.8                 Up        5601
```

---

### **BÆ°á»›c 2: Báº­t Crawler (Terminal 1)**

Má»Ÿ **Terminal 1**, cháº¡y:

```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Crawl nhiá»u tá»‰nh, láº·p má»—i 60 phÃºt, tá»‘i Ä‘a 500 trang/tá»‰nh
python run_continuous_crawler.py --provinces ha-noi ho-chi-minh da-nang --interval 60 --max-page 500
```

**TÃ¹y chá»‰nh tham sá»‘:**
```bash
# Test nhanh (5 trang, 1 tá»‰nh)
python run_continuous_crawler.py --provinces ha-noi --interval 999 --max-page 5

# Production (nhiá»u tá»‰nh, crawl liÃªn tá»¥c)
python run_continuous_crawler.py \
    --provinces ha-noi ho-chi-minh da-nang hai-phong can-tho \
    --interval 60 \
    --max-page 999
```

**Log mong Ä‘á»£i:**
```
INFO - Starting crawler for province: ha-noi
INFO - Scraped from <200 https://...>
INFO - Sent 1 messages to Kafka topic: batdongsan
```

âš ï¸ **Äá»ƒ terminal nÃ y cháº¡y, khÃ´ng Ä‘Ã³ng!**

---

### **BÆ°á»›c 3: Báº­t Kafkaâ†’MinIO Consumer (Terminal 2)**

Má»Ÿ **Terminal 2**, cháº¡y:

```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Láº¥y data tá»« Kafka vÃ  lÆ°u vÃ o MinIO (Data Lake)
python kafka_to_minio.py
```

**Log mong Ä‘á»£i:**
```
INFO - Connecting to MinIO: localhost:9000
INFO - Bucket 'datalake' exists
INFO - Successfully consumed 50 messages
INFO - Saved batch to: raw/province=ha_noi/year=2025/month=12/data_20251215_143022.parquet
INFO - Batch processing time: 2.34s
```

âš ï¸ **Äá»ƒ terminal nÃ y cháº¡y song song vá»›i crawler!**

---

### **BÆ°á»›c 4: Äá»£i Thu Tháº­p Dá»¯ Liá»‡u**

**Xem tiáº¿n Ä‘á»™ trong MinIO Console:**
1. Má»Ÿ browser: http://localhost:9001
2. Login: `minioadmin` / `minioadmin`
3. VÃ o: **Object Browser** â†’ **datalake** â†’ **raw/**
4. Chá»n tá»‰nh â†’ nÄƒm â†’ thÃ¡ng
5. Xem sá»‘ lÆ°á»£ng file `.parquet` vÃ  kÃ­ch thÆ°á»›c

**Thá»i gian khuyáº¿n nghá»‹:**
- **Test nhanh:** 5-10 phÃºt (50-100 records)
- **Dataset vá»«a:** 1-2 giá» (1,000-5,000 records)
- **Dataset lá»›n:** 5-7 ngÃ y (100,000+ records)

**Khi nÃ o dá»«ng:**
- Khi Ä‘Ã£ cÃ³ Ä‘á»§ data Ä‘á»ƒ test ETL
- Terminal 1 Ä‘Ã£ crawl xong cÃ¡c trang
- MinIO Ä‘Ã£ cÃ³ nhiá»u file parquet

---

### **BÆ°á»›c 5: Dá»«ng Crawler**

Khi Ä‘Ã£ cÃ³ Ä‘á»§ data:

```bash
# Terminal 1 & 2: Nháº¥n Ctrl+C Ä‘á»ƒ dá»«ng
# Äá»£i kafka_to_minio.py xá»­ lÃ½ háº¿t buffer trong Kafka
# Khi tháº¥y: "No new messages, waiting..." â†’ Ctrl+C
```

---

### **BÆ°á»›c 6: Cháº¡y ETL Batch Processing**

Má»Ÿ **Terminal 3**, cháº¡y:

```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Option 1: Full ETL (xá»­ lÃ½ toÃ n bá»™ data)
python etl_batch_job.py --mode full

# Option 2: Incremental ETL (chá»‰ xá»­ lÃ½ thÃ¡ng hiá»‡n táº¡i)
python etl_batch_job.py --mode incremental --year 2025 --month 12
```

**Log mong Ä‘á»£i:**
```
INFO - ========================================
INFO - STARTING FULL ETL PIPELINE
INFO - ========================================
INFO - [PHASE 1] Bronze -> Silver (Full Refresh)
INFO - Loaded 12,450 records from Bronze layer
INFO - After deduplication: 11,823 records
INFO - âœ“ Bronze -> Silver completed successfully
INFO - [PHASE 2] Silver -> Gold
INFO - Loaded 11,823 records from Silver layer
INFO - âœ“ Silver -> Gold completed successfully
INFO - ========================================
INFO - ETL PIPELINE COMPLETED SUCCESSFULLY
INFO - Total Duration: 127.45 seconds
INFO - ========================================
```

**Káº¿t quáº£ trong MinIO:**
```
datalake/
â”œâ”€â”€ raw/                    # Bronze (khÃ´ng Ä‘á»•i)
â”œâ”€â”€ silver/                 # Cleaned data
â”‚   â””â”€â”€ year=2025/month=12/
â””â”€â”€ gold/                   # Analytics
    â”œâ”€â”€ district_aggregation/
    â”œâ”€â”€ daily_trends/
    â”œâ”€â”€ province_summary/
    â””â”€â”€ quality_metrics/
```

---

### **BÆ°á»›c 7: Xem Káº¿t Quáº£**

```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Xem káº¿t quáº£ vÃ  export CSV
python view_results.py
```

**Output:**
- ğŸ“Š Sá»‘ lÆ°á»£ng records trong Bronze, Silver, Gold
- ğŸ“ˆ Statistics (giÃ¡ trung bÃ¬nh, diá»‡n tÃ­ch)
- ğŸ† Top districts by price
- ğŸŒ Province summary
- ğŸ’¾ Export CSV option

---

## ï¿½ KIá»‚M TRA Káº¾T QUáº¢

### **1. MinIO Console (Web UI)**
```
URL: http://localhost:9001
Login: minioadmin / minioadmin

Cáº¥u trÃºc data:
datalake/
â”œâ”€â”€ raw/                    # Bronze Layer
â”‚   â””â”€â”€ province=ha_noi/...
â”œâ”€â”€ silver/                 # Silver Layer  
â”‚   â””â”€â”€ year=2025/month=12/
â””â”€â”€ gold/                   # Gold Layer
    â”œâ”€â”€ district_aggregation/
    â”œâ”€â”€ daily_trends/
    â”œâ”€â”€ province_summary/
    â””â”€â”€ quality_metrics/
```

### **2. Python Script**
```bash
# Xem data nhanh
python view_results.py

# Hoáº·c xem trá»±c tiáº¿p vá»›i pandas
python -c "
import pandas as pd
from minio import Minio
client = Minio('localhost:9000', access_key='minioadmin', secret_key='minioadmin', secure=False)
# Download vÃ  xem Silver data
objects = list(client.list_objects('datalake', prefix='silver/', recursive=True))
if objects:
    client.fget_object('datalake', objects[0].object_name, 'temp.parquet')
    df = pd.read_parquet('temp.parquet')
    print(f'Records: {len(df)}')
    print(df.head())
"
```

---

## âš¡ QUICK TEST (5-10 phÃºt)

Äá»ƒ test nhanh pipeline trÆ°á»›c khi cháº¡y production:

```bash
# Terminal 1: Start Docker
docker-compose up -d && timeout /t 30

# Terminal 2: Quick crawl (5 trang)
cd bds
scrapy crawl bds_spider -a province=ha-noi -a max_page=5

# Terminal 3: Kafkaâ†’MinIO
python kafka_to_minio.py
# Äá»£i tháº¥y "Saved batch to..." rá»“i Ctrl+C

# Terminal 4: Run ETL
python etl_batch_job.py --mode full

# View results
python view_results.py
```

---

## ğŸ›‘ Dá»ªNG TOÃ€N Bá»˜

```bash
# Dá»«ng Docker containers
docker-compose down

# Hoáº·c chá»‰ stop (khÃ´ng xÃ³a volumes)
docker-compose stop
```

---

## ğŸ”§ TROUBLESHOOTING

### âŒ Lá»—i: "Connection refused" khi crawler
```bash
# Kiá»ƒm tra Kafka
docker ps | grep kafka
docker logs kafka

# Restart
docker-compose restart kafka zookeeper
```

### âŒ Lá»—i: MinIO "NoSuchBucket"
```bash
# Táº¡o bucket
docker exec -it minio mc alias set local http://localhost:9000 minioadmin minioadmin
docker exec -it minio mc mb local/datalake
```

### âŒ ETL lá»—i: "No such file or directory"
```bash
# Kiá»ƒm tra cÃ³ file trong raw/
python -c "
from minio import Minio
client = Minio('localhost:9000', access_key='minioadmin', secret_key='minioadmin', secure=False)
files = list(client.list_objects('datalake', prefix='raw/', recursive=True))
print(f'Found {len(files)} files')
"
```

### âŒ Import error: "No module named 'pyspark'"
```bash
# CÃ i dependencies
pip install -r requirements.txt

# Hoáº·c cÃ i riÃªng PySpark
pip install pyspark==3.4.3
```

---

## ğŸ“š TÃ€I LIá»†U CHI TIáº¾T

- **[RUN_PIPELINE.md](RUN_PIPELINE.md)** - HÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§ tá»«ng bÆ°á»›c
- **[FILE_STRUCTURE.md](FILE_STRUCTURE.md)** - Giáº£i thÃ­ch táº¥t cáº£ cÃ¡c file
- **[ETL_GUIDE.md](ETL_GUIDE.md)** - Kiáº¿n trÃºc Medallion (Bronze/Silver/Gold)

---

## âœ… CHECKLIST

```bash
# Pre-flight check
[ ] Docker Desktop Ä‘ang cháº¡y
[ ] Python 3.8+ Ä‘Ã£ cÃ i
[ ] pip install -r requirements.txt thÃ nh cÃ´ng

# Pipeline execution
[ ] docker-compose up -d â†’ 5 containers running
[ ] Terminal 1: python run_continuous_crawler.py
[ ] Terminal 2: python kafka_to_minio.py
[ ] MinIO Console cÃ³ files trong raw/
[ ] Ctrl+C cáº£ 2 terminals khi Ä‘á»§ data
[ ] python etl_batch_job.py --mode full
[ ] MinIO Console cÃ³ files trong silver/ vÃ  gold/
[ ] python view_results.py thÃ nh cÃ´ng

# Results
[ ] CSV files exported
[ ] Statistics hiá»ƒn thá»‹ Ä‘Ãºng
[ ] District aggregation cÃ³ data
```

ğŸ‰ **HOÃ€N Táº¤T!**
