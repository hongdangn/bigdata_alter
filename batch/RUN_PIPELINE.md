# ğŸš€ HÆ¯á»šNG DáºªN CHáº Y TOÃ€N Bá»˜ BATCH PIPELINE

## ğŸ“‹ KIáº¾N TRÃšC PIPELINE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CRAWLER   â”‚â”€â”€â”€â–ºâ”‚  KAFKA  â”‚â”€â”€â”€â–ºâ”‚  MinIO  â”‚â”€â”€â”€â–ºâ”‚  ETL BATCH   â”‚
â”‚  (Scrapy)   â”‚    â”‚ (Queue) â”‚    â”‚(Bronze) â”‚    â”‚  (PySpark)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â–¼                      â–¼            â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ BRONZE  â”‚          â”‚ SILVER  â”‚  â”‚  GOLD   â”‚
                              â”‚  (Raw)  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚(Cleaned)â”‚â”€â”€â–ºâ”‚(Analytics)â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ BÆ¯á»šC 1: CHUáº¨N Bá»Š MÃ”I TRÆ¯á»œNG

### 1.1. Kiá»ƒm tra Docker Desktop
```powershell
# Má»Ÿ Docker Desktop hoáº·c kiá»ƒm tra
Get-Process "Docker Desktop" -ErrorAction SilentlyContinue
```

### 1.2. Khá»Ÿi Ä‘á»™ng Docker Services
```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Start all containers
docker-compose up -d

# Äá»£i 30 giÃ¢y Ä‘á»ƒ Kafka khá»Ÿi Ä‘á»™ng hoÃ n toÃ n
timeout /t 30

# Kiá»ƒm tra cÃ¡c container Ä‘ang cháº¡y
docker ps
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
CONTAINER ID   IMAGE                         STATUS         PORTS
xxx            confluentinc/cp-kafka         Up             9092-9093
xxx            confluentinc/cp-zookeeper     Up             2181
xxx            minio/minio                   Up             9000-9001
xxx            elasticsearch:8.18.8          Up             9200,9300
xxx            kibana:8.18.8                 Up             5601
```

### 1.3. Kiá»ƒm tra MinIO
```powershell
# Má»Ÿ MinIO Console
start http://localhost:9001
# Login: minioadmin / minioadmin

# Kiá»ƒm tra bucket 'datalake' Ä‘Ã£ tá»“n táº¡i chÆ°a
# Náº¿u chÆ°a, táº¡o bucket má»›i tÃªn 'datalake'
```

### 1.4. Kiá»ƒm tra Kafka Topic
```bash
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092

# Náº¿u chÆ°a cÃ³ topic 'batdongsan', táº¡o má»›i:
docker exec -it kafka kafka-topics --create \
  --topic batdongsan \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1
```

---

## ğŸ•·ï¸ BÆ¯á»šC 2: CRAWL Dá»® LIá»†U â†’ KAFKA

### Option 1: Crawl Nhiá»u Tá»‰nh (Khuyáº¿n nghá»‹)

**Terminal 1 (PowerShell):**
```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Crawl 5 tá»‰nh lá»›n, má»—i tá»‰nh max 500 trang, láº·p má»—i 60 phÃºt
python run_continuous_crawler.py \
  --provinces ha-noi ho-chi-minh da-nang hai-phong can-tho \
  --interval 60 \
  --max-page 500
```

### Option 2: Crawl Tuáº§n Tá»± CÃ¡c Tá»‰nh

**Terminal 1:**
```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Crawl 10 tá»‰nh, má»—i tá»‰nh 100 trang, cháº¡y 1 láº§n
python run_multi_province.py --max-page 100 --continuous

# Hoáº·c cháº¡y liÃªn tá»¥c (láº·p láº¡i)
python run_multi_province.py --max-page 100 --continuous --interval 120
```

### Option 3: Crawl 1 Tá»‰nh Cá»¥ Thá»ƒ

**Terminal 1:**
```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter\bds

# Crawl chá»‰ HÃ  Ná»™i
scrapy crawl bds_spider -a province=ha-noi -a max_page=999
```

**ğŸ“Š Theo dÃµi log:**
- Báº¡n sáº½ tháº¥y: `Scraped from <200 https://...>`
- Má»—i listing Ä‘Æ°á»£c gá»­i vÃ o Kafka topic `batdongsan`
- Log: `Sent 1 messages to Kafka`

**âš ï¸ Äá»ƒ terminal nÃ y cháº¡y, khÃ´ng Ä‘Ã³ng!**

---

## ğŸ“¦ BÆ¯á»šC 3: KAFKA â†’ MinIO (BRONZE LAYER)

**Terminal 2 (PowerShell má»›i):**
```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Khá»Ÿi Ä‘á»™ng consumer lÆ°u data vÃ o MinIO
python kafka_to_minio.py
```

**ğŸ“Š Theo dÃµi log:**
```
INFO - Connecting to MinIO: localhost:9000
INFO - Bucket 'datalake' exists
INFO - Successfully consumed 50 messages
INFO - Saved batch to: raw/province=ha_noi/year=2025/month=12/data_20251215_143022.parquet
INFO - Batch processing time: 2.34s
```

**Cáº¥u trÃºc dá»¯ liá»‡u trong MinIO:**
```
datalake/
â””â”€â”€ raw/                                    # â† BRONZE LAYER
    â”œâ”€â”€ province=ha_noi/
    â”‚   â””â”€â”€ year=2025/
    â”‚       â””â”€â”€ month=12/
    â”‚           â”œâ”€â”€ data_20251215_100000.parquet
    â”‚           â”œâ”€â”€ data_20251215_103000.parquet
    â”‚           â””â”€â”€ data_20251215_110000.parquet
    â”œâ”€â”€ province=ho_chi_minh/
    â””â”€â”€ province=da_nang/
```

**âš ï¸ Äá»ƒ terminal nÃ y cháº¡y song song vá»›i crawler!**

---

## â¸ï¸ BÆ¯á»šC 4: Äá»¢I THU THáº¬P Äá»¦ Dá»® LIá»†U

### Kiá»ƒm tra dá»¯ liá»‡u hiá»‡n cÃ³

**Option 1: MinIO Console**
```
1. Má»Ÿ: http://localhost:9001
2. Login: minioadmin / minioadmin
3. Object Browser â†’ datalake â†’ raw
4. Xem sá»‘ lÆ°á»£ng file vÃ  kÃ­ch thÆ°á»›c
```

**Option 2: Python Script**
```bash
python monitor.py
```

### Æ¯á»›c lÆ°á»£ng thá»i gian cáº§n crawl

| Má»¥c tiÃªu | Cáº¥u hÃ¬nh | Thá»i gian Æ°á»›c tÃ­nh |
|----------|----------|-------------------|
| **Test nhá»** | 1 tá»‰nh, 10 trang | 5-10 phÃºt |
| **Dataset vá»«a** | 3 tá»‰nh, 100 trang má»—i tá»‰nh | 1-2 giá» |
| **Dataset lá»›n** | 5 tá»‰nh, 500 trang, crawl liÃªn tá»¥c | 5-7 ngÃ y |

**Khuyáº¿n nghá»‹:** Crawl Ã­t nháº¥t **2-3 giá»** Ä‘á»ƒ cÃ³ Ä‘á»§ data demo ETL batch.

### Dá»«ng crawling khi Ä‘á»§ data

```bash
# Dá»«ng crawler (Terminal 1)
Ctrl + C

# Äá»£i kafka_to_minio.py xá»­ lÃ½ háº¿t buffer (Terminal 2)
# Khi tháº¥y: "No new messages, waiting..."
# CÃ³ thá»ƒ Ctrl + C Ä‘á»ƒ dá»«ng
```

---

## âš™ï¸ BÆ¯á»šC 5: ETL BATCH (BRONZE â†’ SILVER â†’ GOLD)

### 5.1. Cháº¡y Full ETL Pipeline (Láº§n Ä‘áº§u)

**Terminal 3 (PowerShell má»›i):**
```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Xá»­ lÃ½ TOÃ€N Bá»˜ dá»¯ liá»‡u trong Bronze â†’ Silver â†’ Gold
python etl_batch_job.py --mode full
```

**ğŸ“Š Log mong Ä‘á»£i:**
```
INFO - Starting FULL ETL Pipeline...
INFO - ========================================
INFO - PHASE 1: Bronze -> Silver Transformation
INFO - ========================================
INFO - Reading raw data from: s3a://datalake/raw/
INFO - Total raw records: 12,450
INFO - After deduplication: 11,823 records
INFO - Saved to: s3a://datalake/silver/
INFO - 
INFO - ========================================
INFO - PHASE 2: Silver -> Gold Analytics
INFO - ========================================
INFO - Reading silver data from: s3a://datalake/silver/
INFO - Creating district aggregations...
INFO - Creating daily trends...
INFO - Saved to: s3a://datalake/gold/
INFO - 
INFO - ETL PIPELINE COMPLETED SUCCESSFULLY
INFO - Total Duration: 127.45 seconds
```

### 5.2. Cháº¡y Incremental ETL (HÃ ng ngÃ y)

```bash
# Chá»‰ xá»­ lÃ½ data thÃ¡ng 12/2025
python etl_batch_job.py --mode incremental --year 2025 --month 12
```

**Khi nÃ o dÃ¹ng Incremental?**
- âœ… ÄÃ£ cháº¡y Full ETL 1 láº§n
- âœ… Chá»‰ muá»‘n xá»­ lÃ½ data má»›i trong thÃ¡ng hiá»‡n táº¡i
- âœ… Cháº¡y hÃ ng ngÃ y Ä‘á»ƒ update Gold layer

### 5.3. Kiá»ƒm tra káº¿t quáº£ trong MinIO

```
datalake/
â”œâ”€â”€ raw/                      # BRONZE (khÃ´ng Ä‘á»•i)
â”‚   â””â”€â”€ province=.../...
â”‚
â”œâ”€â”€ silver/                   # SILVER (Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch)
â”‚   â””â”€â”€ year=2025/
â”‚       â””â”€â”€ month=12/
â”‚           â””â”€â”€ part-00000-xxx.parquet
â”‚
â””â”€â”€ gold/                     # GOLD (aggregated)
    â”œâ”€â”€ district_aggregation/
    â”‚   â””â”€â”€ part-00000-xxx.parquet
    â”œâ”€â”€ daily_trends/
    â”‚   â””â”€â”€ year=2025/
    â”‚       â””â”€â”€ month=12/
    â”‚           â””â”€â”€ part-00000-xxx.parquet
    â”œâ”€â”€ province_summary/
    â”‚   â””â”€â”€ part-00000-xxx.parquet
    â””â”€â”€ quality_metrics/
        â””â”€â”€ part-00000-xxx.parquet
```

---

## ğŸ“Š BÆ¯á»šC 6: XEM & PHÃ‚N TÃCH Káº¾T QUáº¢

### 6.1. Xem Parquet files trong Python

**Táº¡o file `view_results.py`:**
```python
import pandas as pd
from minio import Minio

# Connect MinIO
client = Minio("localhost:9000",
               access_key="minioadmin",
               secret_key="minioadmin",
               secure=False)

# Download Silver data
objects = list(client.list_objects("datalake", prefix="silver/", recursive=True))
print(f"ğŸ“„ Found {len(objects)} Silver files")

# Download file Ä‘áº§u tiÃªn
obj = objects[0]
client.fget_object("datalake", obj.object_name, "temp_silver.parquet")

# Äá»c vÃ  xem
df = pd.read_parquet("temp_silver.parquet")
print(f"\nâœ… Silver Layer: {len(df)} records")
print(df.head(10))
print(df.columns.tolist())

# Download Gold - District Aggregation
client.fget_object("datalake", "gold/district_aggregation/part-00000-xxx.parquet", "temp_gold.parquet")
df_gold = pd.read_parquet("temp_gold.parquet")
print(f"\nâœ… Gold Layer - District Agg: {len(df_gold)} records")
print(df_gold.head())
```

### 6.2. Query vá»›i PySpark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("QueryResults") \
    .config("spark.jars.packages", 
            "org.apache.hadoop:hadoop-aws:3.3.4,"
            "com.amazonaws:aws-java-sdk-bundle:1.12.262") \
    .getOrCreate()

# Äá»c Silver
df_silver = spark.read.parquet("s3a://datalake/silver/")
df_silver.createOrReplaceTempView("listings")

# Query
spark.sql("""
    SELECT 
        province_clean,
        COUNT(*) as count,
        AVG(price_vnd) as avg_price,
        AVG(square_m2) as avg_square
    FROM listings
    WHERE price_vnd > 0 AND square_m2 > 0
    GROUP BY province_clean
    ORDER BY count DESC
""").show()
```

### 6.3. Export CSV Ä‘á»ƒ xem trong Excel

```python
import pandas as pd
from minio import Minio

client = Minio("localhost:9000", 
               access_key="minioadmin",
               secret_key="minioadmin",
               secure=False)

# Download Gold - District Aggregation
client.fget_object("datalake", 
                   "gold/district_aggregation/part-00000-xxx.parquet",
                   "district_agg.parquet")

df = pd.read_parquet("district_agg.parquet")
df.to_csv("district_aggregation.csv", index=False, encoding='utf-8-sig')
print("âœ… Exported to: district_aggregation.csv")
```

---

## ğŸ”„ BÆ¯á»šC 7: Tá»° Äá»˜NG HÃ“A (SCHEDULING)

### Option 1: Cháº¡y tá»± Ä‘á»™ng hÃ ng ngÃ y

```bash
cd D:\HUST\20251\IT4931_bigdata\bigdata_alter

# Schedule: Má»—i ngÃ y 2:00 AM cháº¡y incremental ETL
python etl_scheduler.py --daily --daily-time 02:00
```

### Option 2: Task Scheduler Windows

1. Má»Ÿ **Task Scheduler** (Láº­p lá»‹ch tÃ¡c vá»¥)
2. Create Basic Task:
   - Name: `ETL Batch Daily`
   - Trigger: Daily at 2:00 AM
   - Action: Start a program
   - Program: `python`
   - Arguments: `etl_batch_job.py --mode incremental`
   - Start in: `D:\HUST\20251\IT4931_bigdata\bigdata_alter`

---

## ğŸ› ï¸ TROUBLESHOOTING

### âŒ Lá»—i: "Connection refused" khi crawl
```bash
# Kiá»ƒm tra Kafka
docker ps | grep kafka
docker logs kafka

# Restart Kafka
docker-compose restart kafka zookeeper
```

### âŒ Lá»—i: MinIO "NoSuchBucket"
```bash
# Táº¡o bucket
docker exec -it minio mc alias set local http://localhost:9000 minioadmin minioadmin
docker exec -it minio mc mb local/datalake
```

### âŒ ETL lá»—i: "No such file or directory"
```bash
# Kiá»ƒm tra data trong MinIO
# Äáº£m báº£o cÃ³ file .parquet trong raw/
```

### âŒ PySpark lá»—i Java heap space
```bash
# TÄƒng memory cho Spark
export PYSPARK_DRIVER_MEMORY=4g
export PYSPARK_EXECUTOR_MEMORY=4g
```

---

## ğŸ“ˆ Káº¾T QUáº¢ MONG Äá»¢I

Sau khi cháº¡y xong toÃ n bá»™ pipeline, báº¡n sáº½ cÃ³:

### Bronze Layer (Raw Data)
- âœ… File Parquet Ä‘Æ°á»£c partition theo: `province/year/month`
- âœ… Dá»¯ liá»‡u thÃ´ tá»« Kafka, chÆ°a xá»­ lÃ½
- âœ… KÃ­ch thÆ°á»›c: ~100 KB - 5 MB/file tÃ¹y batch size

### Silver Layer (Cleaned Data)
- âœ… ÄÃ£ parse giÃ¡, diá»‡n tÃ­ch, ngÃ y Ä‘Äƒng
- âœ… ÄÃ£ deduplicate (loáº¡i trÃ¹ng láº·p)
- âœ… ÄÃ£ chuáº©n hÃ³a text vÃ  Ä‘á»‹a chá»‰
- âœ… CÃ³ thÃªm cÃ¡c trÆ°á»ng: `price_vnd`, `square_m2`, `price_per_m2`, `quality_score`

### Gold Layer (Analytics)
- âœ… `district_aggregation/`: GiÃ¡ trung bÃ¬nh theo quáº­n/huyá»‡n
- âœ… `daily_trends/`: Sá»‘ lÆ°á»£ng tin Ä‘Äƒng theo ngÃ y
- âœ… `province_summary/`: Tá»•ng há»£p theo tá»‰nh
- âœ… `quality_metrics/`: ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng dá»¯ liá»‡u

---

## ğŸ¯ QUICK START (TÃ“M Táº®T)

```bash
# 1. Start Docker
docker-compose up -d
timeout /t 30

# 2. Start Crawler (Terminal 1)
python run_continuous_crawler.py --provinces ha-noi ho-chi-minh --interval 60 --max-page 100

# 3. Start Kafka->MinIO (Terminal 2)
python kafka_to_minio.py

# 4. Äá»£i 1-2 giá», sau Ä‘Ã³ Ctrl+C cáº£ 2 terminal

# 5. Run ETL Batch (Terminal 3)
python etl_batch_job.py --mode full

# 6. View results
python view_results.py
```

---

## ğŸ“ SUPPORT

Náº¿u gáº·p lá»—i, kiá»ƒm tra:
1. `docker ps` - Táº¥t cáº£ containers Ä‘ang cháº¡y
2. `docker logs kafka` - Kafka logs
3. `docker logs minio` - MinIO logs
4. `monitor.py` - Sá»‘ lÆ°á»£ng records Ä‘Ã£ crawl

**Good luck! ğŸš€**
