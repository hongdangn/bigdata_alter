# ğŸ”„ Batch Processing Guide

## Tá»•ng quan

Há»‡ thá»‘ng Ä‘Ã£ Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘á»ƒ há»— trá»£ **cáº£ Streaming vÃ  Batch Processing** theo kiáº¿n trÃºc Lambda.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATA SOURCES (Scrapy Crawler)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SPEED LAYER    â”‚    â”‚  BATCH LAYER    â”‚
â”‚ (Streaming)    â”‚    â”‚  (Batch)        â”‚
â”‚                â”‚    â”‚                 â”‚
â”‚ â€¢ Kafka        â”‚    â”‚ â€¢ JSON/CSV      â”‚
â”‚ â€¢ Spark        â”‚    â”‚ â€¢ Spark Batch   â”‚
â”‚   Streaming    â”‚    â”‚ â€¢ Scheduler     â”‚
â”‚ â€¢ Real-time    â”‚    â”‚ â€¢ Historical    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   SERVING LAYER      â”‚
        â”‚   â€¢ Elasticsearch    â”‚
        â”‚   â€¢ Kibana          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ File má»›i Ä‘Æ°á»£c táº¡o

### 1. **spark_batch.py** - Spark Batch Processing
- Xá»­ lÃ½ dá»¯ liá»‡u tá»« file JSON/CSV/Parquet
- Há»— trá»£ replay data tá»« Kafka
- Táº¡o aggregated statistics
- LÆ°u káº¿t quáº£ vÃ o Elasticsearch + Parquet

### 2. **batch_scheduler.py** - Batch Job Scheduler
- LÃªn lá»‹ch cháº¡y batch jobs tá»± Ä‘á»™ng
- Máº·c Ä‘á»‹nh: Daily (2 AM), Hourly stats, Weekly cleanup
- Sá»­ dá»¥ng APScheduler

### 3. **export_to_batch.py** - Export Pipeline
- Scrapy pipeline Ä‘á»ƒ export data ra file
- Há»— trá»£ JSON vÃ  CSV
- Batch size cÃ³ thá»ƒ cáº¥u hÃ¬nh

### 4. **unified_pipeline.py** - Unified Manager
- Quáº£n lÃ½ táº¥t cáº£ pipelines tá»« má»™t nÆ¡i
- Interactive menu
- Health monitoring

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### **BÆ°á»›c 1: CÃ i Ä‘áº·t dependencies má»›i**

```bash
pip install -r requirements.txt
```

### **BÆ°á»›c 2: Táº¡o thÆ° má»¥c cho batch data**

```bash
mkdir -p data/batch_input
mkdir -p data/processed_batch
```

### **BÆ°á»›c 3: Cáº¥u hÃ¬nh Scrapy Ä‘á»ƒ export batch files**

Má»Ÿ `bds/batdongsan/settings.py` vÃ  thÃªm:

```python
# Batch export configuration
BATCH_OUTPUT_DIR = './data/batch_input'
BATCH_FILE_FORMAT = 'json'  # hoáº·c 'csv'
BATCH_SIZE = 1000  # sá»‘ items má»—i file
```

Cáº­p nháº­t ITEM_PIPELINES:

```python
ITEM_PIPELINES = {
    'batdongsan.pipelines.BatdongsanPipeline': 300,
    'batdongsan.pipelines.PushToKafka': 400,  # Streaming
    'export_to_batch.ExportToBatchFile': 500,  # Batch (thÃªm dÃ²ng nÃ y)
}
```

---

## ğŸ“‹ CÃ¡c mode hoáº¡t Ä‘á»™ng

### **Mode 1: Chá»‰ Streaming (nhÆ° hiá»‡n táº¡i)**

```bash
# Terminal 1: Start Spark Streaming
python spark_streaming.py

# Terminal 2: Start Crawler
cd bds
scrapy crawl bds_spider
```

### **Mode 2: Chá»‰ Batch Processing**

```bash
# Cháº¡y batch ngay láº­p tá»©c
python spark_batch.py

# Hoáº·c cháº¡y theo lá»‹ch
python batch_scheduler.py
```

### **Mode 3: Hybrid - Cáº£ Streaming vÃ  Batch**

```bash
# Sá»­ dá»¥ng Unified Manager
python unified_pipeline.py

# Chá»n option 4 trong menu: "Start Both (Hybrid)"
```

### **Mode 4: Sá»­ dá»¥ng Unified Manager (Recommended)**

```bash
python unified_pipeline.py
```

Menu:
```
1. Start Streaming Pipeline (real-time)
2. Start Batch Processing (scheduled)
3. Run Immediate Batch Processing
4. Start Both (Hybrid)
5. Start Crawler Only
6. Check Health
7. Stop All
8. Exit
```

---

## ğŸ¯ Use Cases

### **Use Case 1: Real-time + Historical**

**Ká»‹ch báº£n:** Báº¡n muá»‘n vá»«a xá»­ lÃ½ real-time vá»«a phÃ¢n tÃ­ch historical data

```bash
# 1. Start streaming pipeline
python spark_streaming.py

# 2. Start batch scheduler (background)
python batch_scheduler.py &

# 3. Start crawler
cd bds
scrapy crawl bds_spider
```

### **Use Case 2: Reprocess Historical Data**

**Ká»‹ch báº£n:** Báº¡n Ä‘Ã£ cÃ³ data trong Kafka vÃ  muá»‘n xá»­ lÃ½ láº¡i

```python
# Trong spark_batch.py, Ä‘á»•i SOURCE_TYPE:
SOURCE_TYPE = "kafka_replay"  # Replay tá»« Kafka
SOURCE_PATH = None  # KhÃ´ng cáº§n path

# Cháº¡y batch
python spark_batch.py
```

### **Use Case 3: Scheduled Daily Aggregation**

**Ká»‹ch báº£n:** TÃ­nh toÃ¡n thá»‘ng kÃª má»—i ngÃ y lÃºc 2 AM

```bash
# Chá»‰nh batch_scheduler.py (Ä‘Ã£ config sáºµn)
python batch_scheduler.py
```

### **Use Case 4: Export to Data Lake**

**Ká»‹ch báº£n:** LÆ°u data vÃ o Parquet Ä‘á»ƒ phÃ¢n tÃ­ch sau

```python
# spark_batch.py tá»± Ä‘á»™ng lÆ°u vÃ o Parquet
# Partition theo province Ä‘á»ƒ query nhanh hÆ¡n
write_to_parquet(processed_df, OUTPUT_PATH, partition_by=["province"])
```

---

## âš™ï¸ Configuration

### **Spark Batch Processing**

Sá»­a trong `spark_batch.py`:

```python
# Data source
SOURCE_TYPE = "json"  # Options: 'json', 'csv', 'parquet', 'kafka_replay'
SOURCE_PATH = "./data/batch_input/*.json"

# Output
ES_INDEX = "batdongsan"
OUTPUT_PATH = "./data/processed_batch"
```

### **Batch Scheduler**

Sá»­a trong `batch_scheduler.py`:

```python
# Daily batch at 2 AM
self.scheduler.add_job(
    self.daily_batch_processing,
    trigger=CronTrigger(hour=2, minute=0),
    ...
)

# Hourly stats update
self.scheduler.add_job(
    self.hourly_statistics_update,
    trigger=CronTrigger(minute=0),  # Every hour
    ...
)
```

---

## ğŸ“Š Batch Processing Features

### **1. Data Deduplication**
- Tá»± Ä‘á»™ng loáº¡i bá» duplicates dá»±a trÃªn `link`

### **2. Data Validation**
- Lá»c records khÃ´ng há»£p lá»‡
- Kiá»ƒm tra required fields

### **3. Statistics Generation**
- Thá»‘ng kÃª theo province
- Thá»‘ng kÃª theo district
- LÆ°u vÃ o index riÃªng: `batdongsan_stats_province`

### **4. Multiple Output Formats**
- Elasticsearch (searchable)
- Parquet (data lake, analytics)
- Partitioned by province (query optimization)

---

## ğŸ” Monitoring

### **Check Batch Job Status**

```bash
# Check Elasticsearch indices
curl http://localhost:9200/_cat/indices?v

# Check Parquet files
ls -lh data/processed_batch/

# Check batch input files
ls -lh data/batch_input/
```

### **View Statistics**

```bash
# Province statistics
curl http://localhost:9200/batdongsan_stats_province/_search?pretty

# Or trong Kibana:
# http://localhost:5601
```

---

## ğŸ› ï¸ Troubleshooting

### **Problem: Batch job khÃ´ng cháº¡y**

```bash
# Check scheduler logs
python batch_scheduler.py

# Hoáº·c run manual
python spark_batch.py
```

### **Problem: File khÃ´ng Ä‘Æ°á»£c táº¡o**

```bash
# Check export pipeline config
cd bds
grep -A 5 "ITEM_PIPELINES" batdongsan/settings.py

# Check thÆ° má»¥c tá»“n táº¡i
mkdir -p data/batch_input
```

### **Problem: Spark batch cháº­m**

```python
# TÄƒng parallelism trong spark_batch.py
spark = SparkSession.builder \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.default.parallelism", "100") \
    ...
```

---

## ğŸ“ˆ So sÃ¡nh Streaming vs Batch

| Feature | Streaming | Batch |
|---------|-----------|-------|
| **Latency** | < 1 second | Minutes to hours |
| **Data Source** | Kafka (real-time) | Files, Kafka replay |
| **Use Case** | Real-time monitoring | Historical analysis |
| **Resource** | Always running | Scheduled/on-demand |
| **Complexity** | Higher | Lower |
| **Aggregations** | Limited | Rich (SQL) |

---

## ğŸ“ Best Practices

1. **Streaming cho real-time:** DÃ¹ng cho dashboard, alerts
2. **Batch cho analytics:** DÃ¹ng cho reports, ML training
3. **Hybrid approach:** Combine both Ä‘á»ƒ cÃ³ best of both worlds
4. **Data Lake:** LÆ°u Parquet Ä‘á»ƒ re-process sau nÃ y
5. **Partitioning:** Partition by date/province Ä‘á»ƒ query nhanh

---

## ğŸ“ Next Steps

1. **Test batch processing:**
   ```bash
   python spark_batch.py
   ```

2. **Setup scheduler:**
   ```bash
   python batch_scheduler.py
   ```

3. **Try unified manager:**
   ```bash
   python unified_pipeline.py
   ```

4. **Monitor results in Kibana:**
   ```
   http://localhost:5601
   ```

---

## ğŸ¤ Support

Náº¿u gáº·p váº¥n Ä‘á»:
- Check logs trong terminal
- Verify Docker services: `docker-compose ps`
- Check Elasticsearch: `curl localhost:9200/_cluster/health`
