# H∆∞·ªõng d·∫´n Crawl li√™n t·ª•c

## V·∫•n ƒë·ªÅ: T·∫°i sao crawler t·ª± ƒë·ªông d·ª´ng?

### Nguy√™n nh√¢n ch√≠nh:

1. **HTTP Cache ƒë∆∞·ª£c b·∫≠t vƒ©nh vi·ªÖn** (`HTTPCACHE_EXPIRATION_SECS = 0`)
   - T·∫•t c·∫£ requests ƒë·ªÅu ƒë∆∞·ª£c serve t·ª´ cache
   - Kh√¥ng c√≥ request th·ª±c s·ª± n√†o ƒë∆∞·ª£c g·ª≠i ƒëi
   - Spider nhanh ch√≥ng "ho√†n th√†nh" v√¨ ch·ªâ ƒë·ªçc cache

2. **Spider thi·∫øt k·∫ø ƒë·ªÉ ch·∫°y m·ªôt l·∫ßn**
   - Khi h·∫øt trang ho·∫∑c h·∫øt link ‚Üí d·ª´ng
   - Kh√¥ng c√≥ c∆° ch·∫ø crawl ƒë·ªãnh k·ª≥

### ƒê√£ fix:

‚úÖ T·∫Øt HTTP cache trong production (`HTTPCACHE_ENABLED = False`)  
‚úÖ Th√™m cache expiration time n·∫øu b·∫≠t l·∫°i  
‚úÖ T·∫°o scripts ƒë·ªÉ ch·∫°y crawler li√™n t·ª•c

---

## C√°ch s·ª≠ d·ª•ng

### üöÄ Option 1: X√≥a cache v√† ch·∫°y l·∫°i (ƒë∆°n gi·∫£n nh·∫•t)

```bash
# X√≥a cache c≈©
rm -rf bds/.scrapy/httpcache/

# Ho·∫∑c tr√™n Windows
# Remove-Item -Recurse -Force bds\.scrapy\httpcache\

# Ch·∫°y crawler
cd bds
scrapy crawl bds_spider -a province=ha-noi -a max_page=100
```

### üîÑ Option 2: Ch·∫°y nhi·ªÅu t·ªânh (script ƒë∆°n gi·∫£n)

```bash
# Ch·∫°y m·ªôt l·∫ßn cho H√† N·ªôi, max 100 trang
python run_multi_province.py --provinces ha-noi --max-page 100

# Ch·∫°y nhi·ªÅu t·ªânh
python run_multi_province.py --provinces ha-noi ho-chi-minh da-nang --max-page 50

# Ch·∫°y li√™n t·ª•c, m·ªói 60 ph√∫t m·ªôt l·∫ßn
python run_multi_province.py --provinces ha-noi ho-chi-minh --continuous --interval 60

# Ch·∫°y t·∫•t c·∫£ t·ªânh l·ªõn (15 t·ªânh)
python run_multi_province.py --max-page 100
```

### ‚öôÔ∏è Option 3: Crawler n√¢ng cao v·ªõi nhi·ªÅu t√≠nh nƒÉng

```bash
# Ch·∫°y li√™n t·ª•c cho H√† N·ªôi, interval 30 ph√∫t
python run_continuous_crawler.py --provinces ha-noi --interval 30 --max-page 100

# Nhi·ªÅu t·ªânh
python run_continuous_crawler.py \
    --provinces ha-noi ho-chi-minh da-nang hai-phong \
    --interval 60 \
    --max-page 100
```

### üê≥ Option 4: Ch·∫°y v·ªõi Docker (khuy·∫øn ngh·ªã cho production)

T·∫°o file `docker-compose.crawler.yml`:

```yaml
version: '3.8'

services:
  crawler:
    build: 
      context: .
      dockerfile: Dockerfile.crawler
    container_name: crawler
    depends_on:
      - kafka
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9093
      - CRAWL_INTERVAL=60
      - MAX_PAGE=100
      - PROVINCES=ha-noi,ho-chi-minh,da-nang
    networks:
      - batdongsan-network
    restart: unless-stopped
```

---

## C·∫•u h√¨nh HTTP Cache

### Trong `bds/batdongsan/settings.py`:

```python
# Development: B·∫≠t cache ƒë·ªÉ ti·∫øt ki·ªám bandwidth
HTTPCACHE_ENABLED = True
HTTPCACHE_EXPIRATION_SECS = 3600  # 1 gi·ªù

# Production: T·∫Øt cache ƒë·ªÉ l·∫•y d·ªØ li·ªáu m·ªõi
HTTPCACHE_ENABLED = False
HTTPCACHE_EXPIRATION_SECS = 3600  # Ch·ªâ d√πng khi b·∫≠t
```

### X√≥a cache:

```bash
# Linux/Mac
rm -rf bds/.scrapy/httpcache/

# Windows PowerShell
Remove-Item -Recurse -Force bds\.scrapy\httpcache\

# Windows CMD
rmdir /s /q bds\.scrapy\httpcache\
```

---

## Ki·ªÉm tra logs

### D·∫•u hi·ªáu crawl t·ª´ cache:

```
'httpcache/hit': 1624,  # ‚Üê T·∫•t c·∫£ t·ª´ cache
'downloader/response_count': 1624,
'elapsed_time_seconds': 58.16,  # ‚Üê Qu√° nhanh, ch·ªâ ~1 ph√∫t
```

### D·∫•u hi·ªáu crawl th·ª±c s·ª±:

```
'downloader/response_bytes': 15814634,  # ‚Üê C√≥ download
'httpcache/miss': 1200,  # ‚Üê C√≥ requests m·ªõi
'httpcache/hit': 424,
'elapsed_time_seconds': 600.45,  # ‚Üê M·∫•t th·ªùi gian h·ª£p l√Ω
```

---

## Tham s·ªë Spider

```bash
scrapy crawl bds_spider \
    -a province=ha-noi \        # T·ªânh c·∫ßn crawl
    -a min_page=1 \             # Trang b·∫Øt ƒë·∫ßu
    -a max_page=100 \           # Trang k·∫øt th√∫c
    -a jump_to_page=50          # Nh·∫£y ƒë·∫øn trang c·ª• th·ªÉ (optional)
```

---

## Danh s√°ch t·ªânh ph·ªï bi·∫øn

```python
PROVINCES = [
    'ha-noi',
    'ho-chi-minh',
    'da-nang',
    'hai-phong',
    'can-tho',
    'bien-hoa',
    'vung-tau',
    'nha-trang',
    'hue',
    'hai-duong',
    'nam-dinh',
    'thai-nguyen',
    'vinh',
    'quy-nhon',
    'da-lat',
]
```

---

## Lu·ªìng d·ªØ li·ªáu ho√†n ch·ªânh

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Crawler   ‚îÇ ‚îÄ‚îÄ> Scrapy crawl website
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Kafka    ‚îÇ ‚îÄ‚îÄ> Queue messages
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ‚îÄ> kafka_to_minio.py ‚îÄ‚îÄ> MinIO Data Lake
       ‚îÇ                          (raw/province=xxx/year=xxxx/month=xx/)
       ‚îÇ
       ‚îî‚îÄ‚îÄ> spark_streaming.py ‚îÄ‚îÄ> Elasticsearch
                                    (Kibana visualization)
```

---

## Monitoring

### Ki·ªÉm tra Kafka c√≥ data kh√¥ng:

```bash
docker exec -it kafka kafka-console-consumer \
    --bootstrap-server localhost:9093 \
    --topic batdongsan \
    --from-beginning \
    --max-messages 10
```

### Ki·ªÉm tra MinIO c√≥ data kh√¥ng:

```python
from minio import Minio

client = Minio("localhost:9000", "minioadmin", "minioadmin", secure=False)
objects = list(client.list_objects("datalake", prefix="raw/", recursive=True))
print(f"Total objects: {len(objects)}")
for obj in objects[:10]:
    print(f"  - {obj.object_name} ({obj.size} bytes)")
```

### Ki·ªÉm tra Elasticsearch:

```bash
curl http://localhost:9200/batdongsan/_count
```

---

## Tips

1. **Crawl th·ª≠ v·ªõi max_page nh·ªè** tr∆∞·ªõc (10-20 trang) ƒë·ªÉ test
2. **T·∫Øt cache** khi mu·ªën d·ªØ li·ªáu m·ªõi nh·∫•t
3. **D√πng continuous mode** cho production
4. **Monitor Kafka lag** ƒë·ªÉ ƒë·∫£m b·∫£o consumer theo k·ªãp
5. **Backup MinIO** ƒë·ªãnh k·ª≥

---

## Troubleshooting

### Crawler v·∫´n d·ª´ng nhanh?

1. X√≥a cache: `rm -rf bds/.scrapy/httpcache/`
2. Ki·ªÉm tra `HTTPCACHE_ENABLED = False`
3. Restart t·ª´ ƒë·∫ßu

### Kh√¥ng c√≥ data trong Kafka?

1. Ki·ªÉm tra Kafka ƒëang ch·∫°y: `docker ps`
2. Xem crawler logs c√≥ l·ªói kh√¥ng
3. Test Kafka connection

### MinIO kh√¥ng nh·∫≠n data?

1. Ch·∫°y `kafka_to_minio.py`
2. Ki·ªÉm tra MinIO credentials
3. Xem logs consumer
