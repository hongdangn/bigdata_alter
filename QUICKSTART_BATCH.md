# Quick Start Guide: Batch Processing

## ğŸš€ Khá»Ÿi Ä‘á»™ng nhanh Batch Processing

### BÆ°á»›c 1: CÃ i Ä‘áº·t dependencies
```bash
pip install apscheduler
```

### BÆ°á»›c 2: Táº¡o thÆ° má»¥c
```bash
mkdir data
mkdir data\batch_input
mkdir data\processed_batch
```

### BÆ°á»›c 3: Chá»n mode hoáº¡t Ä‘á»™ng

#### Option A: Cháº¡y Batch ngay láº­p tá»©c
```bash
python spark_batch.py
```

#### Option B: Cháº¡y Batch theo lá»‹ch
```bash
python batch_scheduler.py
```

#### Option C: Sá»­ dá»¥ng Unified Manager (Recommended)
```bash
python unified_pipeline.py
# Chá»n option trong menu
```

### BÆ°á»›c 4: KÃ­ch hoáº¡t Batch Export trong Scrapy (Optional)

Má»Ÿ file `bds/batdongsan/settings.py` vÃ  uncomment pháº§n batch config:

```python
# Uncomment nhá»¯ng dÃ²ng nÃ y:
ITEM_PIPELINES = {
    'batdongsan.pipelines.BatdongsanPipeline': 300,
    'batdongsan.pipelines.PushToKafka': 400,
    'export_to_batch.ExportToBatchFile': 500,  # â† ThÃªm dÃ²ng nÃ y
}
BATCH_OUTPUT_DIR = '../data/batch_input'
BATCH_FILE_FORMAT = 'json'
BATCH_SIZE = 1000
```

### BÆ°á»›c 5: Kiá»ƒm tra káº¿t quáº£

```bash
# Check batch files
dir data\batch_input

# Check processed files
dir data\processed_batch

# Check Elasticsearch
curl http://localhost:9200/batdongsan/_count
curl http://localhost:9200/batdongsan_stats_province/_search?pretty
```

---

## ğŸ“‹ Workflow Examples

### Example 1: Cháº¡y full pipeline (Streaming + Batch)

```bash
# Terminal 1: Start Docker services
docker-compose up -d

# Terminal 2: Start Streaming
python spark_streaming.py

# Terminal 3: Start Batch Scheduler
python batch_scheduler.py

# Terminal 4: Start Crawler
cd bds
scrapy crawl bds_spider
```

### Example 2: Chá»‰ xá»­ lÃ½ Batch tá»« files cÃ³ sáºµn

```bash
# 1. CÃ³ data trong data/batch_input/*.json
# 2. Run batch processing
python spark_batch.py
# 3. Check results trong Elasticsearch vÃ  Parquet
```

### Example 3: Replay data tá»« Kafka

Sá»­a trong `spark_batch.py`:
```python
SOURCE_TYPE = "kafka_replay"
SOURCE_PATH = None
```

Cháº¡y:
```bash
python spark_batch.py
```

---

## âš¡ Lá»‡nh há»¯u Ã­ch

```bash
# Install dependencies
pip install -r requirements.txt

# Check Docker services
docker-compose ps

# View Elasticsearch indices
curl http://localhost:9200/_cat/indices?v

# Count documents
curl http://localhost:9200/batdongsan/_count

# View province stats
curl http://localhost:9200/batdongsan_stats_province/_search?pretty

# Monitor crawler
cd bds
scrapy crawl bds_spider --logfile=crawler.log

# Check batch scheduler logs
python batch_scheduler.py > scheduler.log 2>&1 &
tail -f scheduler.log
```

---

## ğŸ¯ Recommended Setup

Äá»ƒ cÃ³ tráº£i nghiá»‡m tá»‘t nháº¥t, cháº¡y theo thá»© tá»±:

```bash
# 1. Start infrastructure
docker-compose up -d
timeout /t 30  # Wait 30 seconds

# 2. Create Kafka topic (if not exists)
docker exec kafka kafka-topics --create --topic batdongsan --bootstrap-server localhost:9093 --partitions 3 --replication-factor 1

# 3. Start unified pipeline manager
python unified_pipeline.py

# 4. Chá»n option 4: "Start Both (Hybrid)"
```

Xong! Há»‡ thá»‘ng sáº½ cháº¡y cáº£ streaming vÃ  batch processing Ä‘á»“ng thá»i.

---

## ğŸ“Š Kiá»ƒm tra káº¿t quáº£

- **Kibana Dashboard:** http://localhost:5601
- **Elasticsearch:** http://localhost:9200
- **Batch files:** `data/batch_input/`
- **Processed Parquet:** `data/processed_batch/`

Äá»c thÃªm chi tiáº¿t trong `BATCH_PROCESSING_GUIDE.md`
